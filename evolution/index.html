<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Exploring Direct Policy Search via Evolutionary Algorithms (2022) | Michael E. Van Huffel</title> <meta name="author" content="Michael E. Van Huffel"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/android-chrome-512x512.png?b1de276ff87c52fed453e8272baf2888"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://majkevh.github.io/evolution/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Michael </span>E. Van Huffel</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">projects</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/homology/">Persistent homology of the cosmic web</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/evolution/">Direct Policy Search via Evolutionary Algorithms</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume.pdf">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h2 class="post-title">Exploring Direct Policy Search via Evolutionary Algorithms (2022)</h2> <p class="post-description"></p> </header> <article> <h4 id="introduction">Introduction</h4> <p><strong>Black Box Optimization via Evolutionary Algorithms.</strong> Evolution strategies (ES) belong to a class of nature-inspired optimization methods known as evolutionary algorithms, which were initially developed by <a href="https://link.springer.com/article/10.1023/A:1015059928466" rel="external nofollow noopener" target="_blank">H. Beyer in 1989</a>. These algorithms utilize adaptation, selection, and recombination of individuals, representing potential solutions, to iteratively improve solutions. It’s crucial to emphasize that while these algorithms draw inspiration from biological evolution, their mathematical formulations are highly abstract, making it more appropriate to regard evolution strategies as a <a href="https://arxiv.org/abs/1703.03864" rel="external nofollow noopener" target="_blank">class of black-box stochastic optimization techniques designed for tackling continuous non-linear or non-convex problems</a>.</p> <p>Given a black-box function \(f: D \subset \mathbb{R}^{N_x} \rightarrow \mathbb{R}\), the focus lies on the optimization problem:</p> \[\boldsymbol{z}^{\star}:=\underset{\boldsymbol{z} \in \Omega}{\arg \max } f(\boldsymbol{z}),\] <p>with \(\Omega \subset \mathbb{R}^{N_x}\). Rather than directly solving this objective, one can seek a solution to the following objective:</p> \[\boldsymbol{\vartheta}^{\star}:=\underset{\boldsymbol{\vartheta} \in \Omega}{\arg \max } \mathbb{E}_{p(\boldsymbol{z} \mid \boldsymbol{\vartheta})} f(\boldsymbol{z}),\] <p>where \(p(\boldsymbol{z} \mid \boldsymbol{\vartheta})\) represents a distribution on \(D\), parameterized by the vector \(\boldsymbol{\vartheta} \in \Omega\). If \(p(\boldsymbol{z} \mid \boldsymbol{\vartheta})\) can sample points near the maximum of \(f(\boldsymbol{z})\), these <a href="https://arxiv.org/abs/1905.10474" rel="external nofollow noopener" target="_blank">two formulations yield the same solution</a>. The stochastic formulation of the optimization problem provides a solid foundation for analyzing derivative-free algorithms and allows for the integration of auxiliary information.</p> <p>The general structure of the ES algorithm follows these steps: (1) <em>Sample Population</em>: In each generation, the ES Algorithm samples individuals from the population based on a selected probability density function. (2) <em>Evaluate Individuals and Update Parameters</em>: Each individual is evaluated, and the problem parameters are subsequently adjusted to increase the likelihood of sampling individuals closer to the maximum of the chosen objective function in comparison to previous generations.</p> <p><strong>Reinforcement Learning.</strong> Reinforcement learning problems are characterized by closed-loop systems where agents learn how to map states to actions with the aim of maximizing cumulative rewards. The states represent observations obtained from the environment, while the actions are generated based on the agent’s policy, which depends on the state. Consequently, <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf" rel="external nofollow noopener" target="_blank">to maximize cumulative rewards during training, the learning system must strike a balance between exploration and exploitation</a>. In policy-based reinforcement learning, the policy \(\pi_{\boldsymbol\vartheta}(\,\cdot\,|s_t)\) is parameterized by a vector \(\boldsymbol\vartheta\in\mathbb{R}^d\), determining the actions of the learning agent as a function of the states as mentioned by <a href="https://arxiv.org/abs/1806.09460" rel="external nofollow noopener" target="_blank">Recht et al. (2019)</a>.</p> <p>In the realm of mathematical optimization, our goal is to solve the following problem:</p> \[\text{max}_{\boldsymbol\vartheta\in\mathbb{R}^d}\mathbb{E}_{a_t\sim\pi_{\boldsymbol\vartheta}(\,\cdot\,|s_t)} \left[\sum_{t=0}^{T-1}\gamma^tr(s_t,a_t)\big|s_0=s\right]\] <p>Here, \(\gamma\) is a discounting factor, and \(r(s_t,a_t)\) represents the reward obtained in state \(s_t\) after taking action \(a_t\). Since we are primarily interested in controlling continuous locomotion tasks, \(r(s_t, a_t)\) denotes the cumulative sum of discounted returns observed by the agent.</p> <p>The optimization problem outlined in in the previous equation closely resembles the problem described in the above section when considering black-box optimization.<a href="https://arxiv.org/abs/1703.03864" rel="external nofollow noopener" target="_blank">This approach is commonly referred to as direct policy search or neuro-evolution when applied to neural networks</a>.</p> <h4 id="contributions-and-methods">Contributions and Methods</h4> <p>In this comprehensive study, I explored two distinct methodologies aimed at addressing the aforementioned black-box optimization problem. The first approach focused on obtaining solutions through precise Maximum Likelihood (EM) and Maximum A Posteriori estimations (MAP). In contrast to the CMA-ES algorithm proposed by <a href="https://arxiv.org/abs/1604.00772" rel="external nofollow noopener" target="_blank">Hansen (2016)</a>, my approach aimed to implement algorithms that adhere closely to the theoretical foundations, without employing an evolutionary path or making modifications to the covariance matrix update, such as the “rank-\(\mu\)” update.</p> <p>The second approach was grounded in a gradient-based strategy. In this approach, I developed two algorithms: Natural Gradient Descent based (NGA) and a Newton-Raphson based algorithm (SNM), taking inspiration from the work done respectively by <a href="https://arxiv.org/abs/2109.00375" rel="external nofollow noopener" target="_blank">L. Tan (2021)</a> and <a href="https://ieeexplore.ieee.org/abstract/document/4788533" rel="external nofollow noopener" target="_blank">Haley et al. (1984)</a>. It’s important to note that for the Natural Gradient method, I utilized normalized gradient ascent updates to ensure more stable and efficient optimization.</p> <p>To maintain positive-semidefiniteness at each iteration in the gradient-based methods, I updated the Cholesky decomposition of the covariance matrix.</p> <p>Taking inspiration from the seminal works of <a href="https://arxiv.org/abs/1703.03864" rel="external nofollow noopener" target="_blank">Salimans et al (2017)</a> and <a href="https://arxiv.org/abs/1803.07055" rel="external nofollow noopener" target="_blank">Mania et al. (2018)</a>, I conducted a thorough investigation into the applicability of Evolution Strategies for direct policy search in handling continuous problems within the classical <a href="https://gymnasium.farama.org/environments/mujoco/" rel="external nofollow noopener" target="_blank">MuJoCo</a> benchmark tasks. The selected benchmark tasks included the <a href="https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/" rel="external nofollow noopener" target="_blank">Inverted-pendulum</a>, <a href="https://gymnasium.farama.org/environments/mujoco/swimmer/" rel="external nofollow noopener" target="_blank">Swimmer</a>, <a href="https://gymnasium.farama.org/environments/mujoco/hopper/" rel="external nofollow noopener" target="_blank">Hopper</a>, <a href="https://gymnasium.farama.org/environments/mujoco/walker2d/" rel="external nofollow noopener" target="_blank">Walker2D</a>, and <a href="https://gymnasium.farama.org/environments/mujoco/half_cheetah/" rel="external nofollow noopener" target="_blank">Half Cheetah</a> locomotion challenges. While the Inverted-pendulum, Swimmer, and Hopper tasks had relatively low dimensionality action and observation space, I prudently imposed constraints on the covariance matrix for the Walker2D and Half Cheetah tasks to enhance computational efficiency, ensuring it remained diagonal.</p> <p>For representing policies, I opted for linear policies with batch normalization. Throughout the optimization process, I employed individual selection, assigning a weight close to zero to the bottom-performing \(25\%\) of the population at each generational step. Additionally, I maintained a discount factor of \(\gamma = 1\) and limited the maximum number of steps per environment to \(T = 1000\). Subsequently, I conducted a comprehensive performance comparison of my algorithms with two established benchmarks: <a href="https://arxiv.org/abs/1502.05477" rel="external nofollow noopener" target="_blank">Trust Region Policy Optimization</a> (TRPO) and the Evolutionary Algorithm from <a href="https://arxiv.org/abs/1703.03864" rel="external nofollow noopener" target="_blank">Salimans et al.(2017)</a> (ES-Sal).</p> <h4 id="results-and-observations">Results and Observations</h4> <figure> <img src="/assets/img/es.png" alt="Results" width="750px" height="auto"> <figcaption> Caption: Results of the research. For each task, the dimensionality D of the problem is reported near its name (D is equal to the dimension of action space multiplied by dimension of observation space), as well as the type of covariance matrix utilized (full or diagonal). </figcaption> </figure> <p>The figures above show that most of the algorithms proposed in this work are capable of training policies for simple Mujoco locomotion tasks. The results obtained also make it clear that all four algorithms outperform both the TRPO and Evolutionary Strategies (ES) algorithms in the Inverted Pendulum task. For the Swimmer task, it is evident that all algorithms are outperformed by the ES-Sal algorithm. However, the SNM and EM algorithms still outperform the TRPO algorithm. On the other hand, the other algorithms either perform worse (NGA) or fail to achieve any rewards (MAP).</p> <p>In the Halfcheetah task, only the SNM, EM, and NGA algorithms managed to reach the performance threshold set by <a href="https://arxiv.org/abs/1703.03864" rel="external nofollow noopener" target="_blank">Salimans et al.(2017)</a>. Nevertheless, our algorithms required approximately ten times more cumulative time steps compared to the ES-Sal and TRPO algorithms.</p> <p>For the remaining locomotion tasks (Hopper and Walker2D), all my algorithms could only reach about \(25\%\) of the predefined threshold. This inability to control these tasks can be attributed to two factors. Firstly, the linearity of the policies used was insufficient to fully capture the complex relationships between actions and states. In <a href="https://arxiv.org/abs/1703.03864" rel="external nofollow noopener" target="_blank">Salimans et al.(2017)</a>, a two-layer neural network with 64 neurons per layer was employed to establish a more intricate and nonlinear mapping between state and action. Secondly, even the use of a diagonal covariance matrix did not facilitate the learning of these intricate locomotion tasks.</p> <h4 id="conclusion">Conclusion</h4> <p>This study aimed to develop straightforward algorithms for effective performance on common continuous control locomotion tasks in MuJoCo. While algorithms like MAP, SNM, and EM demonstrated state-of-the-art sample efficiency on certain basic benchmark control problems when employing linear policies, my findings shed light on potential limitations associated with a strictly theoretical approach in algorithm derivation. Notably, the absence of an evolutionary path, the “rank-\(\mu\)” update, and the failure to scale the gradient step, as discussed in <a href="https://arxiv.org/abs/1803.07055" rel="external nofollow noopener" target="_blank">Mania et al. (2018)</a>, can hinder algorithm convergence to a state-of-the-art level in certain scenarios. These insights underscore the importance of striking a balance between theoretical underpinnings and practical considerations when developing algorithms for reinforcement learning in complex tasks.</p> <p>The code for this work is available <a href="https://github.com/majkevh/eth-evolution-strategies-rl" rel="external nofollow noopener" target="_blank">here</a>.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Michael E. Van Huffel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 12, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>